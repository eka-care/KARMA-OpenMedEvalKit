{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Add the project root to Python path so we can import karma\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to DuckDB at cache.db\n",
      "✅ DuckDB persistent storage connected\n"
     ]
    }
   ],
   "source": [
    "from karma.models.indic_conformer import IndicConformerASR, INDIC_CONFORMER_MULTILINGUAL_META\n",
    "from karma.eval_datasets.indicvoices_r_dataset import IndicVoicesRDataset\n",
    "from karma.metrics.common_metrics import WERMetric, CERMetric\n",
    "from karma.benchmark import Benchmark\n",
    "from karma.metrics.asr_wer_processor import ASRTextProcessor\n",
    "from karma.cache.cache_manager import CacheManager\n",
    "\n",
    "\n",
    "model = IndicConformerASR(model_name_or_path=\"ai4bharat/indic-conformer-600m-multilingual\")\n",
    "processor = ASRTextProcessor(use_glm=True, use_num2text=True, use_punc=True, use_lowercasing=True, language=\"hi\")\n",
    "dataset = IndicVoicesRDataset(language=\"Hindi\", postprocessors=[processor])\n",
    "mertric_configs = [\n",
    "    {\n",
    "        \"metric\": WERMetric(metric_name=\"wer\"),\n",
    "        \"processors\": []\n",
    "    },\n",
    "    {\n",
    "        \"metric\": CERMetric(metric_name=\"cer\"),\n",
    "        \"processors\": []\n",
    "    }\n",
    "]\n",
    "cache_manager = CacheManager(model_config=INDIC_CONFORMER_MULTILINGUAL_META, dataset_name=dataset.dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = Benchmark(model=model, dataset=dataset, cache_manager=cache_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Benchmark.evaluate() got an unexpected keyword argument 'metric_configs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mbenchmark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric_configs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmertric_configs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Benchmark.evaluate() got an unexpected keyword argument 'metric_configs'"
     ]
    }
   ],
   "source": [
    "benchmark.evaluate(metric_configs=mertric_configs, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
